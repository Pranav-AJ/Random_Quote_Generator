{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiAefo3G5BPe",
        "outputId": "b2e63b7f-d90c-45b0-babb-21b5573dd47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now Scrapinghttp://quotes.toscrape.com//page/1\n",
            "Now Scrapinghttp://quotes.toscrape.com//page/2/\n",
            "Now Scrapinghttp://quotes.toscrape.com//page/3/\n",
            "Now Scrapinghttp://quotes.toscrape.com//page/4/\n",
            "Now Scrapinghttp://quotes.toscrape.com//page/5/\n",
            "Now Scrapinghttp://quotes.toscrape.com//page/6/\n",
            "Now Scrapinghttp://quotes.toscrape.com//page/7/\n",
            "Now Scrapinghttp://quotes.toscrape.com//page/8/\n",
            "Now Scrapinghttp://quotes.toscrape.com//page/9/\n",
            "Now Scrapinghttp://quotes.toscrape.com//page/10/\n",
            "Here's a quote:  \n",
            "“You believe lies so you eventually learn to trust no one but yourself.”\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from csv import writer\n",
        "from time import sleep\n",
        "from random import choice\n",
        "\n",
        "# list to store scraped data\n",
        "all_quotes = []\n",
        "\n",
        "# this part of the url is constant\n",
        "base_url = \"http://quotes.toscrape.com/\"\n",
        "\n",
        "# this part of the url will keep changing\n",
        "url = \"/page/1\"\n",
        "\n",
        "while url:\n",
        "\n",
        "    # concatenating both urls\n",
        "    # making request\n",
        "    res = requests.get(f\"{base_url}{url}\")\n",
        "    print(f\"Now Scraping{base_url}{url}\")\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    # extracting all elements\n",
        "    quotes = soup.find_all(class_=\"quote\")\n",
        "\n",
        "    for quote in quotes:\n",
        "        all_quotes.append({\n",
        "            \"text\": quote.find(class_=\"text\").get_text(),\n",
        "            \"author\": quote.find(class_=\"author\").get_text(),\n",
        "            \"bio-link\": quote.find(\"a\")[\"href\"]\n",
        "        })\n",
        "    next_btn = soup.find(class_=\"next\")\n",
        "    url = next_btn.find(\"a\")[\"href\"] if next_btn else None\n",
        "    sleep(2)\n",
        "\n",
        "quote = choice(all_quotes)\n",
        "\n",
        "print(\"Here's a quote:  \")\n",
        "print(quote[\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tj-D10TiFIeW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}